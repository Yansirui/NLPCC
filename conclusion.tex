\section{Conclusion}
\label{sec:con}
This study proposes a novel framework called W2CL, which aims to better transfer general-purpose PLMs such as BERT and RoBERTa to specific domains, thereby improving the performance of the model in sentence classification tasks. First, we propose a domain knowledge extraction method based on ChatGPT. This method can extract high-quality domain knowledge from raw texts easily in different domains. Next, we introduce a multi-task learning approach that includes three tasks: WWM, contrastive learning, and word classification. This approach allows better integration of domain knowledge into the model. Our experimental results demonstrate that our proposed W2CL method outperforms other methods in sentence classification tasks across four domains, regardless of whether BERT or RoBERTa is used as the basic model. This also indicates that the W2CL method proposed in this study has general applicability.
