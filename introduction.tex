\section{Introduction}
\label{sec:intro}
In recent years, mainstream sentence classification tasks have largely relied on deep representation learning methods, especially PLMs such as BERT~\cite{bert}, RoBERTa~\cite{roberta}, and T5~\cite{t5}. Although the vector representations from these models capture richer textual features compared to traditional methods, simple fine-tuning is insufficient for integrating domain-specific knowledge into language models for domain-specific sentence classification tasks. Therefore, we propose the W2CL framework to effectively incorporate domain knowledge into the language models to improve the performance of these models in domain-specific tasks.

%将知识融合进模型是有效的
Numerous studies have demonstrated that integrating effective knowledge with PLMs can enhance the performance of PLMs on certain downstream tasks. LIBERT~\cite{libert} adds a lexical relationship classification task to the original BERT pre-training tasks to help the model acquire richer semantic information. SenseBERT~\cite{sensebert} adds a part-of-speech layer to the BERT model, that is, it adds part-of-speech information of words (such as noun.food and noun.state, etc.) to the original input, and uses the representation vector after integrating part-of-speech information for masking tasks and part-of-speech classification tasks. SKEP~\cite{skep} integrates sentiment knowledge (sentiment words, sentiment polarity, and aspect-sentiment pairs) into the model by designing three types of sentiment analysis tasks. Sentiprompt~\cite{sentiprompt} integrates sentiment knowledge by constructing different paradigm templates and masking aspects, polarities, and opinions in the templates, and designing tasks to predict the masked tokens. LET~\cite{let} integrates all classification definitions of HowNet entities into the input, uses the original input and classification information for masking tasks, and finally integrates the knowledge contained in HowNet into the model. KEAR~\cite{kear} directly faces the multiple-choice task, integrates the knowledge relationship of questions and options, the dictionary’s definition knowledge of questions and options, and the knowledge of annotated training samples into the model, improving the model’s performance in the commonsense knowledge question answering task.

The aforementioned methods can enhance the performance of models in various downstream tasks. However, these methods have two issues: First, knowledge cannot be extracted from the raw corporas, which means that acquiring knowledge is challenging; Second, the training frameworks of the aforementioned methods are complex, requiring intricate training procedures to match the corresponding forms of knowledge.

%为什么要用ChatGPT去抽取知识
With the development of large language models (LLMs)~\cite{bert,lm5,lm1,roberta,lm6,lm2,lm4,lm3}, particularly ChatGPT~\cite{gpt}, which has attracted significant attention from NLP researchers, some studies~\cite{gptuse2,gptuse1,gptuse5,gptuse4,gptuse3} have shown that ChatGPT has demonstrated impressive performance, outperforming many models even in zero-shot settings. As a result, for issue 1, we utilize ChatGPT to acquire domain knowledge from raw corporas, a process that is both simple and results in high-quality knowledge. 

Meanwhile, some studies have demonstrated that contrastive learning has superior capabilities in the field of NLP. SimCSE~\cite{simcse} uses the randomness of the dropout layer to construct positive samples, treating other samples in the same batch as negative samples. Building on SimCSE,   ESimCSE~\cite{esimcse} uses random word repetition to construct positive samples. PromptBERT~\cite{promptbert} applies multiple templates that do not affect semantics within the same sample, treating samples reconstructed with different templates as positive samples. BGE~\cite{bge} treats the question and its corresponding answer as positive samples, and other samples in the same batch as negative samples. For domain-specific tasks, domain vocabulary contains more domain-specific information compared to sentences. Therefore, for issue 2, we propose word-level contrastive learning and word classification, which together form the proposed W2CL framework.

To better validate the effectiveness of our proposed method, we need to test it on datasets from multiple domains. However, sentence classification datasets in the Chinese domain are not sufficiently abundant. Therefore, we will contribute our proprietary domain-specific datasets for future research and development, which include text retrieval datasets from four domains and sentence classification datasets from two domains.

Overall, our contributions can be summarized from four perspectives. 1) We propose a novel framework called W2CL, which can better transfer general models to specific domains. 2) We propose a novel and simple general method for knowledge extraction and integration. The knowledge extraction method based on ChatGPT and the knowledge integration method based on word-level contrastive learning and word classification are both effective and scalable. 3) Our experimental results demonstrate that the proposed method can enhance the performance of various language models across multiple domains and tasks, indicating that the W2CL framework is both effective and generalizable. 4) For domain-specific tasks, we contribute six task datasets to support future research and development.


