\section{Introdction}
\label{sec:intro}
In recent years, mainstream text classification tasks have largely relied on deep representation learning methods, especially PLMs such as BERT\cite{bert}, RoBERTa\cite{roberta}, and T5\cite{t5}. Although the vector representations from these models capture richer textual features compared to traditional methods, simple fine-tuning is insufficient for integrating domain-specific knowledge into language models for domain-specific text classification. Therefore, we propose the W2CL framework to effectively incorporate domain knowledge into the language model.

With the development of large language models(LLMs)\cite{bert,lm5,lm1,roberta,lm6,lm2,lm4,lm3}, particularly ChatGPT\cite{gpt}, which has attracted significant attention from NLP researchers, some studies\cite{gptuse2,gptuse1,gptuse5,gptuse4,gptuse3} have shown that ChatGPT has demonstrated impressive performance, outperforming many models even in zero-shot settings. Therefore, we leverage the powerful capabilities of ChatGPT to generate the necessary domain knowledge for our W2CL framework, such as domain-specific vocabulary and word classifications.

Meanwhile, numerous studies have demonstrated that integrating effective knowledge with PLMs can enhance the performance of these models on certain downstream tasks.LIBERT\cite{libert} adds a lexical relationship classification task to the original BERT pre-training tasks to help the model acquire richer semantic information. SenseBERT\cite{sensebert} adds a part-of-speech layer to the BERT model, that is, it adds part-of-speech information of words (such as noun.food and noun.state, etc.) to the original input, and uses the representation vector after integrating part-of-speech information for masking tasks and part-of-speech classification tasks.SKEP\cite{skep} integrates sentiment knowledge (sentiment words, sentiment polarity, and aspect-sentiment pairs) into the model by designing three types of sentiment analysis tasks. Sentiprompt\cite{sentiprompt} integrates sentiment knowledge by constructing different paradigm templates and masking aspects, polarities, and opinions in the templates, and designing tasks to predict the masked tokens. LET\cite{let} integrates all classification definitions of HowNet entities into the input, uses the original input and classification information for masking tasks, and finally integrates the knowledge contained in HowNet into the model. KEAR\cite{kear} directly faces the multiple-choice task, integrates the knowledge relationship of questions and options, the dictionary’s definition knowledge of questions and options, and the knowledge of annotated training samples into the model, improving the model’s performance in the commonsense knowledge question answering task.

The aforementioned methods combine knowledge with pre-training tasks to enhance the performance of models in various downstream tasks. However, these methods have two issues: First, the form of knowledge is fixed and difficult to acquire; Second, in order to utilize the knowledge, new neural network layers need to be designed, which increases the training cost.

For issue 1, we utilize ChatGPT to acquire domain knowledge, a process that is both simple and results in high-quality knowledge. For issue 2, we employ the W2CL framework to integrate this domain knowledge, which requires only the addition of a classification layer, thereby consuming minimal computational resources.

In the field of NLP, recent popular contrastive learning methods typically construct positive and negative samples at the sentence level. SimCSE\cite{simcse} uses the randomness of the dropout layer to construct positive samples, treating other samples in the same batch as negative samples. The training objective is to minimize the vector distance between the positive samples and the current sample, while maximizing the vector distance between the negative samples and the current sample.Building on SimCSE, ESimCSE\cite{esimcse} uses the principle that word repetition generally does not change the meaning of a sentence to construct positive samples.PromptBERT\cite{promptbert} applies multiple templates that do not affect semantics within the same sample, treating samples reconstructed with different templates as positive samples.BGE\cite{bge} directly targets downstream paragraph retrieval tasks, treating the question and its corresponding answer as positive samples, and other samples in the same batch as negative samples. It has shown excellent performance in Chinese paragraph retrieval tasks.For domain-specific tasks, domain vocabulary contains more domain-specific information compared to sentences. However, the aforementioned contrastive learning methods are all based on sentences. Therefore, we propose a word-level contrastive learning method and also introduce a word classification task to provide additional contextual understanding, further boosting the model's performance in domain tasks.

Our contributions can be summarized from three perspectives. 1) We propose a novel framework called W2CL, which can better transfer general models to specific domains. 2) We propose a novel and simple general method for knowledge extraction and integration. The knowledge extraction method based on ChatGPT and the knowledge integration method based on word-level contrastive learning and word classification are both effective and scalable. 3) Our experimental results demonstrate that the proposed method can enhance the performance of various language models across multiple domains and tasks, indicating that the W2CL framework is both effective and generalizable.


