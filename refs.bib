@inproceedings{bert,
	author       = {Jacob Devlin and
	Ming{-}Wei Chang and
	Kenton Lee and
	Kristina Toutanova},
	title        = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
	Understanding},
	booktitle    = {Proceedings of the 2019 Conference of the North American Chapter of
	the Association for Computational Linguistics: Human Language Technologies,
	{NAACL-HLT} 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long
	and Short Papers)},
	pages        = {4171--4186},
	year         = {2019}
}
@article{roberta,
	title={Roberta: A robustly optimized bert pretraining approach},
	author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
	journal={arXiv preprint arXiv:1907.11692},
	year={2019}
}
@article{t5,
	title={Exploring the limits of transfer learning with a unified text-to-text transformer},
	author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
	journal={Journal of machine learning research},
	volume={21},
	number={140},
	pages={1--67},
	year={2020}
}
@inproceedings{libert,
	author       = {Anne Lauscher and
	Ivan Vulic and
	Edoardo Maria Ponti and
	Anna Korhonen and
	Goran Glavas},
	title        = {Specializing Unsupervised Pretraining Models for Word-Level Semantic
	Similarity},
	booktitle    = {Proceedings of the 28th International Conference on Computational
	Linguistics, {COLING} 2020, Barcelona, Spain (Online), December 8-13,
	2020},
	pages        = {1371--1383},
	year         = {2020}
}

@inproceedings{lm1,
	title={Supervised and semi-supervised text categorization using LSTM for region embeddings},
	author={Johnson, Rie and Zhang, Tong},
	booktitle={International Conference on Machine Learning},
	pages={526--534},
	year={2016},
	organization={PMLR}
}

@article{lm2,
	title={Improved semantic representations from tree-structured long short-term memory networks},
	author={Tai, Kai Sheng and Socher, Richard and Manning, Christopher D},
	journal={arXiv preprint arXiv:1503.00075},
	year={2015}
}

@inproceedings{lm3,
	title={Long short-term memory over recursive structures},
	author={Zhu, Xiaodan and Sobihani, Parinaz and Guo, Hongyu},
	booktitle={International conference on machine learning},
	pages={1604--1612},
	year={2015},
	organization={PMLR}
}

@article{lm4,
	title={Attention is all you need},
	author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
	journal={Advances in neural information processing systems},
	volume={30},
	year={2017}
}

@article{lm5,
	title={Deberta: Decoding-enhanced bert with disentangled attention},
	author={He, Pengcheng and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu},
	journal={arXiv preprint arXiv:2006.03654},
	year={2020}
}

@article{lm6,
	title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
	author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
	journal={arXiv preprint arXiv:1910.01108},
	year={2019}
}

@article{gpt,
	title={Improving language understanding by generative pre-training},
	author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
	year={2018},
	publisher={OpenAI}
}

@article{gptuse1,
	title={Putting chatgptâ€™s medical advice to the (turing) test. medRxiv},
	author={Nov, Oded and Singh, Nina and Mann, Devin M},
	journal={Preprint posted online January},
	volume={24},
	year={2023}
}

@article{gptuse2,
	title={Deid-gpt: Zero-shot medical text de-identification by gpt-4},
	author={Liu, Zhengliang and Huang, Yue and Yu, Xiaowei and Zhang, Lu and Wu, Zihao and Cao, Chao and Dai, Haixing and Zhao, Lin and Li, Yiwei and Shu, Peng and others},
	journal={arXiv preprint arXiv:2303.11032},
	year={2023}
}

@article{gptuse3,
	title={Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face},
	author={Shen, Yongliang and Song, Kaitao and Tan, Xu and Li, Dongsheng and Lu, Weiming and Zhuang, Yueting},
	journal={Advances in Neural Information Processing Systems},
	volume={36},
	year={2024}
}

@article{gptuse4,
	title={Is chatgpt a general-purpose natural language processing task solver?},
	author={Qin, Chengwei and Zhang, Aston and Zhang, Zhuosheng and Chen, Jiaao and Yasunaga, Michihiro and Yang, Diyi},
	journal={arXiv preprint arXiv:2302.06476},
	year={2023}
}

@article{gptuse5,
	title={Towards making the most of chatgpt for machine translation},
	author={Peng, Keqin and Ding, Liang and Zhong, Qihuang and Shen, Li and Liu, Xuebo and Zhang, Min and Ouyang, Yuanxin and Tao, Dacheng},
	journal={arXiv preprint arXiv:2303.13780},
	year={2023}
}

@inproceedings{sensebert,
	author       = {Yoav Levine and
	Barak Lenz and
	Or Dagan and
	Ori Ram and
	Dan Padnos and
	Or Sharir and
	Shai Shalev{-}Shwartz and
	Amnon Shashua and
	Yoav Shoham},
	title        = {SenseBERT: Driving Some Sense into {BERT}},
	booktitle    = {Proceedings of the 58th Annual Meeting of the Association for Computational
	Linguistics, {ACL} 2020, Online, July 5-10, 2020},
	pages        = {4656--4667},
	year         = {2020}
}

@inproceedings{skep,
	author       = {Hao Tian and
	Can Gao and
	Xinyan Xiao and
	Hao Liu and
	Bolei He and
	Hua Wu and
	Haifeng Wang and
	Feng Wu},
	title        = {{SKEP:} Sentiment Knowledge Enhanced Pre-training for Sentiment Analysis},
	booktitle    = {Proceedings of the 58th Annual Meeting of the Association for Computational
	Linguistics, {ACL} 2020, Online, July 5-10, 2020},
	pages        = {4067--4076},
	year         = {2020}
}

@article{sentiprompt,
	author       = {Chengxi Li and
	Feiyu Gao and
	Jiajun Bu and
	Lu Xu and
	Xiang Chen and
	Yu Gu and
	Zirui Shao and
	Qi Zheng and
	Ningyu Zhang and
	Yongpan Wang and
	Zhi Yu},
	title        = {SentiPrompt: Sentiment Knowledge Enhanced Prompt-Tuning for Aspect-Based
	Sentiment Analysis},
	journal      = {CoRR},
	year         = {2021}
}

@inproceedings{let,
	author       = {Boer Lyu and
	Lu Chen and
	Su Zhu and
	Kai Yu},
	title        = {{LET:} Linguistic Knowledge Enhanced Graph Transformer for Chinese
	Short Text Matching},
	booktitle    = {Thirty-Fifth {AAAI} Conference on Artificial Intelligence, {AAAI}
	2021, Thirty-Third Conference on Innovative Applications of Artificial
	Intelligence, {IAAI} 2021, The Eleventh Symposium on Educational Advances
	in Artificial Intelligence, {EAAI} 2021, Virtual Event, February 2-9,
	2021},
	pages        = {13498--13506},
	year         = {2021}
}

@inproceedings{kear,
	author       = {Yichong Xu and
	Chenguang Zhu and
	Shuohang Wang and
	Siqi Sun and
	Hao Cheng and
	Xiaodong Liu and
	Jianfeng Gao and
	Pengcheng He and
	Michael Zeng and
	Xuedong Huang},
	title        = {Human Parity on CommonsenseQA: Augmenting Self-Attention with External
	Attention},
	booktitle    = {Proceedings of the Thirty-First International Joint Conference on
	Artificial Intelligence, {IJCAI} 2022, Vienna, Austria, 23-29 July
	2022},
	pages        = {2762--2768},
	year         = {2022}
}

@inproceedings{simcse,
	author       = {Tianyu Gao and
	Xingcheng Yao and
	Danqi Chen},
	title        = {SimCSE: Simple Contrastive Learning of Sentence Embeddings},
	booktitle    = {Proceedings of the 2021 Conference on Empirical Methods in Natural
	Language Processing, {EMNLP} 2021, Virtual Event / Punta Cana, Dominican
	Republic, 7-11 November, 2021},
	pages        = {6894--6910},
	year         = {2021}
}

@inproceedings{esimcse,
	author       = {Xing Wu and
	Chaochen Gao and
	Liangjun Zang and
	Jizhong Han and
	Zhongyuan Wang and
	Songlin Hu},
	title        = {ESimCSE: Enhanced Sample Building Method for Contrastive Learning
	of Unsupervised Sentence Embedding},
	booktitle    = {Proceedings of the 29th International Conference on Computational
	Linguistics, {COLING} 2022, Gyeongju, Republic of Korea, October 12-17,
	2022},
	pages        = {3898--3907},
	year         = {2022}
}

@inproceedings{promptbert,
	author       = {Ting Jiang and
	Jian Jiao and
	Shaohan Huang and
	Zihan Zhang and
	Deqing Wang and
	Fuzhen Zhuang and
	Furu Wei and
	Haizhen Huang and
	Denvy Deng and
	Qi Zhang},
	title        = {PromptBERT: Improving {BERT} Sentence Embeddings with Prompts},
	booktitle    = {Proceedings of the 2022 Conference on Empirical Methods in Natural
	Language Processing, {EMNLP} 2022, Abu Dhabi, United Arab Emirates,
	December 7-11, 2022},
	pages        = {8826--8837},
	year         = {2022}
}

@article{bge,
	author       = {Shitao Xiao and
	Zheng Liu and
	Peitian Zhang and
	Niklas Muennighof},
	title        = {C-Pack: Packaged Resources To Advance General Chinese Embedding},
	journal      = {CoRR},
	year         = {2023}
}

@inproceedings{tase,
	author       = {Elad Segal and
	Avia Efrat and
	Mor Shoham and
	Amir Globerson and
	Jonathan Berant},
	title        = {A Simple and Effective Model for Answering Multi-span Questions},
	booktitle    = {Proceedings of the 2020 Conference on Empirical Methods in Natural
	Language Processing, {EMNLP} 2020, Online, November 16-20, 2020},
	pages        = {3074--3080},
	year         = {2020}
}

@ARTICLE{macbert,
	author={Cui, Yiming and Che, Wanxiang and Liu, Ting and Qin, Bing and Yang, Ziqing},
	journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 
	title={Pre-Training With Whole Word Masking for Chinese BERT}, 
	year={2021},
	volume={29},
	number={},
	pages={3504-3514},
	keywords={Training;Natural language processing;Representation learning;Predictive models;Pre-trained language model;representation learning;natural language processing},
	doi={10.1109/TASLP.2021.3124365}}

