% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
%
\usepackage[T1]{fontenc}
\usepackage{booktabs}
%\usepackage[colorlinks,linkcolor=black,anchorcolor=black,citecolor=black]{hyperref}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{graphicx}
\usepackage{color}
\usepackage{amsmath}
\usepackage{amssymb}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}
%\urlstyle{rm}
%
\begin{document}
%
\title{W2CL:A Multi-Task Learning Approach to Improve Domain-Specific Sentence Classification through Word Classification and Contrastive Learning}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
%\author{A\inst{1}\orcidID{0000-1111-2222-3333} \and
%B\inst{1}\orcidID{1111-2222-3333-4444} \and
%C\inst{1}\orcidID{2222--3333-4444-5555}}
%
\author{Anonymous}
\institute{Anonymous}
\authorrunning{Anonymous}
\titlerunning{W2CL for Domain Sentence Classification}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
%\institute{Review\\
%\email{email1}\\
%\email{\{abc,lncs\}@uni-heidelberg.de}}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
Sentence classification task plays a crucial role in various NLP tasks. Recent studies have shown that contrastive learning can enhance the representational capability of Pre-trained Language Models (PLMs) and that different methods for constructing positive and negative samples can be applied to various downstream application scenarios. Therefore, in this study, we propose \textbf{W2CL}, a novel multi-task learning framework based on \textbf{W}ord \textbf{C}lassification and \textbf{C}ontrastive \textbf{L}earning, aimed at integrating domain knowledge extracted by ChatGPT from raw corporas into PLMs and improving the performance of PLMs in domain-specific sentence classification tasks. Contrastive learning assists the model in gradually learning the semantic similarity and contextual relevance between words during the training process to enhance its ability to understand text. Word classification provides additional contextual understanding, thereby improving the model's ability to differentiate between different classes within the specific domain. Experiments demonstrate that our multi-task approach significantly outperforms other methods, leading to substantial improvements in domain-specific sentence classification performance. This framework offers a robust solution for adapting general-purpose language models to specialized domains, ensuring better performance and generalization in various domain applications.

\keywords{Sentence Classification  \and Contrastive Learning \and Multi-task Learning.}
\end{abstract}
%
%
%
\input{introduction}
\input{method}
\input{experiments}
\input{analysis}
\input{conclusion}
%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
\bibliographystyle{splncs04}
\bibliography{refs}

\end{document}
